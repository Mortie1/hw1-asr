defaults:
    - model: deepspeech2
    - writer: wandb
    - metrics: example
    - datasets: example
    - dataloader: example
    - transforms: example_only_instance
    - _self_
optimizer:
    _target_: torch.optim.Adam
    lr: 1e-4
lr_scheduler:
    _target_: torch.optim.lr_scheduler.ExponentialLR
    gamma: 0.9995
warmup_scheduler:
    _target_: torch.optim.lr_scheduler.LinearLR
    start_factor: 0.01
    end_factor: 1.0
    total_iters: 1
loss_function:
    _target_: src.loss.CTCLossWrapper
text_encoder:
    _target_: src.text_encoder.CTCTextEncoder
trainer:
    log_step: 20
    n_epochs: 50
    epoch_len: 400
    device_tensors: ["spectrogram", "text_encoded"] # which tensors should be on device (ex. GPU)
    resume_from: null # null or path to the checkpoint dir with *.pth and config.yaml
    from_pretrained: D:\hse\DLA\hw1-asr\saved\same, increased epoch len (val is very long)\model_best.pth
    device: cuda # device name or "auto"
    override: False # if True, will override the previous run with the same name
    monitor: "min val_WER_(Beam)" # "off" or "max/min metric_name", i.e. our goal is to maximize/minimize metric
    save_period: 5 # checkpoint each save_period epochs in addition to the best epoch
    early_stop: ${trainer.n_epochs} # epochs for early stopping
    save_dir: "saved"
    seed: 42
    n_gradient_accumulation_steps: 1
    beam_size: 50
